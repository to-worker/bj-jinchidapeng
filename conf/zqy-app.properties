
########## streaming run model #########
spark.execute.local.model=false

########## streaming #########
spark.kafka.transform.batch.millis.duration=4000

########## streaming and kafka #########
spark.kafka.stream.transform.brokers=test81:9092
# elpTransform
spark.kafka.stream.transform.topics=DATASCHEMA
spark.kafka.stream.transform.auto.offset.reset=largest
# transformClient
spark.kafka.stream.transform.group.id=transConsumeGroup01
# elp modify
spark.kafka.stream.transform.entities.with.links={}
spark.kafka.stream.transform.sequenceid=
spark.kafka.stream.transform.elp.operator=OTHER
spark.kafka.stream.transform.zkqurom=test81:2181

# consumer
spark.kafka.stream.transform.max.partition.fetch.bytes=1048576
spark.kafka.stream.transform.connections.max.idle.ms=540000
spark.kafka.stream.transform.client.id=transConsumeClient
# producer
spark.kafka.elp.data.compression.type=none
spark.kafka.elp.data.batch.size=16384
spark.kafka.elp.data.max.request.size=1048576
spark.kafka.elp.data.connections.max.idle.ms=540000
spark.kafka.elp.data.linger.ms=0

######### streaming process #########
spark.warning.process.partition.num=10

########## kafka and producer
spark.kafka.elp.data.brokers=test81:9092
spark.kafka.elp.data.client.id=transProduceCliet
spark.kafka.elp.data.key.serializer=org.apache.kafka.common.serialization.StringSerializer
spark.kafka.elp.data.value.serializer=org.apache.kafka.common.serialization.StringSerializer
spark.kafka.elp.data.topics=loadstandard_model

########## Persist ELP data in Spark Streaming
spark.kafka.elp.load.batch.millis.duration=4000
spark.kafka.elp.load.brokers=test81:9092
spark.kafka.elp.load.topics=loadstandard_model
# consumer
spark.kafka.elp.load.group.id=loadConsumerGroup01
spark.kafka.elp.load.auto.offset.reset=largest
spark.kafka.elp.load.key.serializer=org.apache.kafka.common.serialization.StringSerializer
spark.kafka.elp.load.value.serializer=org.apache.kafka.common.serialization.StringSerializer
spark.kafka.elp.load.max.partition.fetch.bytes=1048576
spark.kafka.elp.load.connections.max.idle.ms=540000
spark.kafka.elp.load.client.id=loadConsumeClient

#
spark.kafka.elp.load.operator=OTHER
spark.kafka.elp.load.entities.with.links={}


########## Solr ###########
# bigdatacluster01,bigdatacluster02,bigdatacluster03:2181/solr
spark.load.solr.zk.server=test81:2181/solr

########## HBase ###########
# bigdatacluster01,bigdatacluster02,bigdatacluster03
spark.load.hbase.zk.quorum=test81

########## mongodb ###########
spark.streaming.mongodb.host=test81
spark.streaming.mongodb.db=hyjj
spark.streaming.mongodb.db.user=zqykj
spark.streaming.mongodb.db.password=zqykj

########## MySQL ###########
spark.streaming.mysql.url=jdbc:mysql://172.30.6.20:3306/simulate_city
spark.streaming.mysql.username=root
spark.streaming.mysql.password=123456

############################ batch transformer ###########################
# avro files
spark.trans.data.path=/outer/172.30.6.25/extractormanager/backup
spark.trans.fs.defaultFS=hdfs://test81:8020
spark.trans.special.dataschemaid=e4e09730-ff8a-4b56-b2c3-c900833785ed
# taskType: FULL SUB SPE
spark.trans.task.type=SUB
spark.trans.solr.batch.size=20000
spark.trans.hbase.batch.size=20000

# ELP
spark.trans.elp.modelId=standard_model
spark.trans.sequenceId=89224352871229444
spark.trans.entities.with.links={}
